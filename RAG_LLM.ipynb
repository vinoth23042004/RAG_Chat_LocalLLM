{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PART 1: Setup and Installation (Run this first)\n",
        "# ========================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q streamlit langchain langchain-community langchain-ollama\n",
        "!pip install -q pypdf docx2txt unstructured sentence-transformers\n",
        "!pip install -q faiss-cpu langchain-huggingface transformers torch\n",
        "!pip install -q pyngrok\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install and setup Ollama in Colab\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama service in background\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set Ollama models directory to your Google Drive\n",
        "os.environ['OLLAMA_MODELS'] = '/content/drive/MyDrive/Ollama_Models'\n",
        "\n",
        "# Kill any existing Ollama processes\n",
        "!pkill -f ollama\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Ollama server in background with proper output handling\n",
        "print(\"Starting Ollama server...\")\n",
        "with open('/tmp/ollama.log', 'w') as f:\n",
        "    ollama_process = subprocess.Popen(\n",
        "        ['ollama', 'serve'],\n",
        "        stdout=f,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        env=os.environ.copy()\n",
        "    )\n",
        "\n",
        "# Wait and verify server is running\n",
        "time.sleep(10)\n",
        "\n",
        "# Check if Ollama is responding\n",
        "max_retries = 5\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        result = subprocess.run(['ollama', 'list'], capture_output=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úì Ollama server is running!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if i < max_retries - 1:\n",
        "        print(f\"Waiting for Ollama to start... ({i+1}/{max_retries})\")\n",
        "        time.sleep(5)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Ollama may not have started properly. Check /tmp/ollama.log\")\n",
        "        print(\"\\nOllama log (last 20 lines):\")\n",
        "        !tail -20 /tmp/ollama.log\n",
        "\n",
        "# Verify the model files exist\n",
        "model_path = '/content/drive/MyDrive/Ollama_Models'\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"‚úì Found Ollama_Models folder\")\n",
        "    print(f\"Contents: {os.listdir(model_path)}\")\n",
        "\n",
        "    # Check for blobs and manifests\n",
        "    if os.path.exists(os.path.join(model_path, 'blobs')):\n",
        "        print(f\"‚úì Found blobs folder\")\n",
        "    if os.path.exists(os.path.join(model_path, 'manifests')):\n",
        "        print(f\"‚úì Found manifests folder\")\n",
        "        manifests_path = os.path.join(model_path, 'manifests')\n",
        "        for root, dirs, files in os.walk(manifests_path):\n",
        "            print(f\"  Manifest structure: {root}\")\n",
        "            print(f\"  Dirs: {dirs}\")\n",
        "            print(f\"  Files: {files}\")\n",
        "else:\n",
        "    print(\"‚úó Ollama_Models folder not found!\")\n",
        "\n",
        "# List available models\n",
        "print(\"\\nChecking available models...\")\n",
        "!ollama list\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è IMPORTANT: If gemma3:1b is not listed above, we'll need to pull it.\")\n",
        "print(\"The model files in your Drive need to be properly recognized by Ollama.\")\n",
        "print(\"\\nOptions:\")\n",
        "print(\"1. Pull the model: !ollama pull gemma3:1b\")\n",
        "print(\"2. Or use a different model that's available\")\n",
        "\n",
        "print(\"\\n‚úì Setup complete! Now run PART 2 to create the app file.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN8MutwwVO-1",
        "outputId": "8bfb57d9-39f2-4e9e-e9d1-470debf48c95"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Starting Ollama server...\n",
            "‚úì Ollama server is running!\n",
            "‚úì Found Ollama_Models folder\n",
            "Contents: ['blobs', 'manifests']\n",
            "‚úì Found blobs folder\n",
            "‚úì Found manifests folder\n",
            "  Manifest structure: /content/drive/MyDrive/Ollama_Models/manifests\n",
            "  Dirs: ['registry.ollama.ai']\n",
            "  Files: []\n",
            "  Manifest structure: /content/drive/MyDrive/Ollama_Models/manifests/registry.ollama.ai\n",
            "  Dirs: ['library']\n",
            "  Files: []\n",
            "  Manifest structure: /content/drive/MyDrive/Ollama_Models/manifests/registry.ollama.ai/library\n",
            "  Dirs: ['gemma3']\n",
            "  Files: []\n",
            "  Manifest structure: /content/drive/MyDrive/Ollama_Models/manifests/registry.ollama.ai/library/gemma3\n",
            "  Dirs: []\n",
            "  Files: ['1b']\n",
            "\n",
            "Checking available models...\n",
            "NAME         ID              SIZE      MODIFIED       \n",
            "gemma3:1b    8648f39daa8f    815 MB    35 minutes ago    \n",
            "\n",
            "‚ö†Ô∏è IMPORTANT: If gemma3:1b is not listed above, we'll need to pull it.\n",
            "The model files in your Drive need to be properly recognized by Ollama.\n",
            "\n",
            "Options:\n",
            "1. Pull the model: !ollama pull gemma3:1b\n",
            "2. Or use a different model that's available\n",
            "\n",
            "‚úì Setup complete! Now run PART 2 to create the app file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PART 1.5: Pull/Verify Gemma Model (Run after PART 1)\n",
        "# ========================================\n",
        "\n",
        "# Available small models for Colab (sorted by size):\n",
        "# - tinyllama (637 MB) - Fastest, less accurate\n",
        "# - llama3.2:1b (1.3 GB) - Good balance\n",
        "# - phi3:mini (2.3 GB) - Better quality\n",
        "# - gemma3:1b (1.6 GB) - Good balance, what you wanted\n",
        "# - gemma2:2b (1.6 GB) - Better than gemma3:1b\n",
        "\n",
        "MODEL_TO_USE = \"gemma3:1b\"  # Change this if you want a different model\n",
        "\n",
        "print(f\"Checking for {MODEL_TO_USE} model...\")\n",
        "\n",
        "# List current models\n",
        "import subprocess\n",
        "result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
        "models_output = result.stdout\n",
        "\n",
        "if MODEL_TO_USE in models_output:\n",
        "    print(f\"‚úì {MODEL_TO_USE} model found!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è {MODEL_TO_USE} not found. Pulling from Ollama library...\")\n",
        "    print(\"This will take a few minutes...\")\n",
        "    !ollama pull {MODEL_TO_USE}\n",
        "    print(\"‚úì Model downloaded!\")\n",
        "\n",
        "# Verify model is now available\n",
        "!ollama list\n",
        "\n",
        "print(f\"\\n‚úì Model verification complete! Using: {MODEL_TO_USE}\")\n",
        "print(\"Now run PART 2 to create the app file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCneRvBSVSjG",
        "outputId": "56c71d4a-0bcf-4485-f1f1-69cff6c85efa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for gemma3:1b model...\n",
            "‚úì gemma3:1b model found!\n",
            "NAME         ID              SIZE      MODIFIED       \n",
            "gemma3:1b    8648f39daa8f    815 MB    35 minutes ago    \n",
            "\n",
            "‚úì Model verification complete! Using: gemma3:1b\n",
            "Now run PART 2 to create the app file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PART 2: Create the Streamlit App (Run after PART 1.5)\n",
        "# ========================================\n",
        "\n",
        "%%writefile app.py\n",
        "import os\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "# Suppress all warnings before imports\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# Suppress protobuf warnings\n",
        "import logging\n",
        "logging.getLogger('google.protobuf').setLevel(logging.ERROR)\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredWordDocumentLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import shutil\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"gemma3:1b\"  # Model to use (should match PART 1.5)\n",
        "\n",
        "# Try to import HuggingFace embeddings\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    EMBEDDING_TYPE = \"huggingface\"\n",
        "except:\n",
        "    from langchain_ollama import OllamaEmbeddings\n",
        "    EMBEDDING_TYPE = \"ollama\"\n",
        "\n",
        "# Check for GPU availability\n",
        "try:\n",
        "    import torch\n",
        "    GPU_AVAILABLE = torch.cuda.is_available()\n",
        "    DEVICE = 'cuda' if GPU_AVAILABLE else 'cpu'\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    DEVICE = 'cpu'\n",
        "\n",
        "# Function to check if Ollama is running\n",
        "def check_ollama_health():\n",
        "    \"\"\"Check if Ollama server is responding\"\"\"\n",
        "    try:\n",
        "        import subprocess\n",
        "        result = subprocess.run(\n",
        "            ['curl', '-s', 'http://localhost:11434/api/tags'],\n",
        "            capture_output=True,\n",
        "            timeout=2\n",
        "        )\n",
        "        return result.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Check Ollama health on startup\n",
        "if not check_ollama_health():\n",
        "    st.error(\"‚ö†Ô∏è Ollama server is not responding!\")\n",
        "    st.info(\"Please restart the Ollama server in your Colab notebook:\")\n",
        "    st.code(\"\"\"\n",
        "# Run this in a new Colab cell:\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Kill existing Ollama\n",
        "!pkill -f ollama\n",
        "\n",
        "# Start Ollama again\n",
        "os.environ['OLLAMA_MODELS'] = '/content/drive/MyDrive/Ollama_Models'\n",
        "with open('/tmp/ollama.log', 'w') as f:\n",
        "    subprocess.Popen(['ollama', 'serve'], stdout=f, stderr=subprocess.STDOUT)\n",
        "    \"\"\")\n",
        "    st.stop()\n",
        "\n",
        "# Set paths for Google Drive\n",
        "DRIVE_ROOT = '/content/drive/MyDrive'\n",
        "os.environ['OLLAMA_MODELS'] = os.path.join(DRIVE_ROOT, 'Ollama_Models')\n",
        "\n",
        "vector_space_dir = os.path.join(os.getcwd(), \"vector_db\")\n",
        "os.makedirs(vector_space_dir, exist_ok=True)\n",
        "\n",
        "st.set_page_config(page_title=\"RAG ChatBot\", layout=\"centered\")\n",
        "st.title(f\"RAG ChatBot (Langchain + {MODEL_NAME})\")\n",
        "\n",
        "if 'vectorstore' not in st.session_state:\n",
        "    st.session_state['vectorstore'] = None\n",
        "if 'memory' not in st.session_state:\n",
        "    st.session_state['memory'] = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key='answer'\n",
        "    )\n",
        "if 'retriever' not in st.session_state:\n",
        "    st.session_state['retriever'] = None\n",
        "if 'embedding_model' not in st.session_state:\n",
        "    st.session_state['embedding_model'] = None\n",
        "\n",
        "upload_file = st.file_uploader(\"Upload PDF or Word file\", type=[\"pdf\", \"docx\", \"doc\"], key='upload_file')\n",
        "\n",
        "def get_embedding_model():\n",
        "    \"\"\"Initialize embedding model only when needed\"\"\"\n",
        "    if st.session_state['embedding_model'] is None:\n",
        "        with st.spinner(\"Loading embedding model...\"):\n",
        "            try:\n",
        "                # Path to local model in Google Drive\n",
        "                local_model_path = os.path.join(DRIVE_ROOT, \"local_model\")\n",
        "\n",
        "                if os.path.exists(local_model_path):\n",
        "                    st.info(f\"Found local model in Drive, loading on {DEVICE.upper()}...\")\n",
        "                    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "                    st.session_state['embedding_model'] = HuggingFaceEmbeddings(\n",
        "                        model_name=local_model_path,\n",
        "                        model_kwargs={\n",
        "                            'device': DEVICE,\n",
        "                            'trust_remote_code': True\n",
        "                        },\n",
        "                        encode_kwargs={'normalize_embeddings': True}\n",
        "                    )\n",
        "                    st.success(f\"‚úì Using local HuggingFace embeddings on {DEVICE.upper()}\")\n",
        "\n",
        "                elif EMBEDDING_TYPE == \"huggingface\":\n",
        "                    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "                    st.session_state['embedding_model'] = HuggingFaceEmbeddings(\n",
        "                        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                        model_kwargs={\n",
        "                            'device': DEVICE,\n",
        "                            'trust_remote_code': True\n",
        "                        },\n",
        "                        encode_kwargs={'normalize_embeddings': True}\n",
        "                    )\n",
        "                    st.info(f\"‚úì Using HuggingFace embeddings on {DEVICE.upper()}\")\n",
        "                else:\n",
        "                    st.session_state['embedding_model'] = OllamaEmbeddings(\n",
        "                        model=\"nomic-embed-text\",\n",
        "                        base_url=\"http://localhost:11434\"\n",
        "                    )\n",
        "                    st.info(\"‚úì Using Ollama embeddings\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"HuggingFace embedding error: {str(e)}\")\n",
        "                st.warning(\"Trying Ollama embeddings as fallback...\")\n",
        "                try:\n",
        "                    from langchain_ollama import OllamaEmbeddings\n",
        "                    st.session_state['embedding_model'] = OllamaEmbeddings(\n",
        "                        model=\"nomic-embed-text\",\n",
        "                        base_url=\"http://localhost:11434\"\n",
        "                    )\n",
        "                    st.info(\"‚úì Using Ollama embeddings (fallback)\")\n",
        "                except Exception as e2:\n",
        "                    st.error(f\"Failed to initialize embeddings: {str(e2)}\")\n",
        "                    return None\n",
        "\n",
        "    return st.session_state['embedding_model']\n",
        "\n",
        "if upload_file is not None and st.session_state['vectorstore'] is None:\n",
        "    with st.spinner(\"Loading document and creating vector DB....\"):\n",
        "        try:\n",
        "            file_path = os.path.join(os.getcwd(), upload_file.name)\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(upload_file.getbuffer())\n",
        "            st.session_state['file_path'] = file_path\n",
        "\n",
        "            file_extension = os.path.splitext(upload_file.name)[1].lower()\n",
        "            st.info(f\"Processing {file_extension} file...\")\n",
        "\n",
        "            if file_extension == '.pdf':\n",
        "                loader = PyPDFLoader(file_path)\n",
        "            elif file_extension == '.docx':\n",
        "                loader = Docx2txtLoader(file_path)\n",
        "            elif file_extension == '.doc':\n",
        "                loader = UnstructuredWordDocumentLoader(file_path)\n",
        "\n",
        "            documents = loader.load()\n",
        "            st.success(f\"‚úì Loaded {len(documents)} document pages/sections\")\n",
        "\n",
        "            embedding_model = get_embedding_model()\n",
        "\n",
        "            if embedding_model is None:\n",
        "                st.error(\"Failed to initialize embedding model.\")\n",
        "            else:\n",
        "                vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "                vectorstore.save_local(vector_space_dir)\n",
        "                st.session_state['vectorstore'] = vectorstore\n",
        "                st.session_state['retriever'] = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "                st.success(\"‚úì Vector DB Created successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error processing document: {str(e)}\")\n",
        "            import traceback\n",
        "            st.code(traceback.format_exc())\n",
        "\n",
        "# Initialize LLM model\n",
        "# Options: \"gemma3:1b\", \"llama3.2:1b\", \"phi3:mini\", \"tinyllama\"\n",
        "MODEL_NAME = \"gemma3:1b\"  # Change this if you want to use a different model\n",
        "\n",
        "llm = OllamaLLM(\n",
        "    model=MODEL_NAME,\n",
        "    base_url=\"http://localhost:11434\",\n",
        "    temperature=0.7,\n",
        "    timeout=60,\n",
        "    num_gpu=1 if GPU_AVAILABLE else 0\n",
        ")\n",
        "\n",
        "if st.session_state['retriever'] is not None:\n",
        "    user_question = st.text_input(\"Ask your question:\", key='text')\n",
        "\n",
        "    if user_question:\n",
        "        # Check Ollama health before processing\n",
        "        if not check_ollama_health():\n",
        "            st.error(\"‚ö†Ô∏è Ollama server connection lost!\")\n",
        "            st.info(\"Please restart Ollama in your Colab notebook and refresh this page.\")\n",
        "            st.stop()\n",
        "\n",
        "        with st.spinner(\"Thinking....\"):\n",
        "            try:\n",
        "                qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "                    llm=llm,\n",
        "                    retriever=st.session_state['retriever'],\n",
        "                    memory=st.session_state['memory'],\n",
        "                    return_source_documents=False,\n",
        "                    verbose=True\n",
        "                )\n",
        "\n",
        "                result = qa_chain.invoke({\"question\": user_question})\n",
        "\n",
        "                if isinstance(result, dict):\n",
        "                    answer = result.get('answer', result.get('result', 'No answer generated'))\n",
        "                else:\n",
        "                    answer = str(result)\n",
        "\n",
        "                st.markdown(f\"**You:** {user_question}\")\n",
        "                st.markdown(f\"**Bot:** {answer}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = str(e)\n",
        "                if \"Cannot assign requested address\" in error_msg or \"ConnectError\" in error_msg:\n",
        "                    st.error(\"‚ö†Ô∏è Lost connection to Ollama server!\")\n",
        "                    st.info(\"The Ollama server stopped responding. Please restart it:\")\n",
        "                    st.code(\"\"\"\n",
        "# Run in Colab:\n",
        "!pkill -f ollama\n",
        "import subprocess, os, time\n",
        "os.environ['OLLAMA_MODELS'] = '/content/drive/MyDrive/Ollama_Models'\n",
        "with open('/tmp/ollama.log', 'w') as f:\n",
        "    subprocess.Popen(['ollama', 'serve'], stdout=f, stderr=subprocess.STDOUT)\n",
        "time.sleep(10)\n",
        "!ollama list\n",
        "                    \"\"\")\n",
        "                else:\n",
        "                    st.error(f\"Error generating response: {error_msg}\")\n",
        "                    import traceback\n",
        "                    with st.expander(\"Show detailed error\"):\n",
        "                        st.code(traceback.format_exc())\n",
        "\n",
        "def del_vectordb(path):\n",
        "    try:\n",
        "        if os.path.exists(path):\n",
        "            shutil.rmtree(path)\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Could not delete vector DB: {str(e)}\")\n",
        "\n",
        "def del_uploaded_file(path):\n",
        "    try:\n",
        "        if os.path.exists(path) and path:\n",
        "            os.remove(path)\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Could not delete file: {str(e)}\")\n",
        "\n",
        "if st.button(\"Clear Session\"):\n",
        "    st.session_state['memory'].clear()\n",
        "    st.session_state['retriever'] = None\n",
        "    st.session_state['vectorstore'] = None\n",
        "    st.session_state['embedding_model'] = None\n",
        "    del_vectordb(vector_space_dir)\n",
        "    file_p = st.session_state.get('file_path', None)\n",
        "    del_uploaded_file(file_p)\n",
        "    st.session_state['file_path'] = None\n",
        "    for key in ['upload_file', 'text']:\n",
        "        if key in st.session_state:\n",
        "            del st.session_state[key]\n",
        "    st.success('Session, document and VectorDB are cleared')\n",
        "    st.rerun()\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Configuration\")\n",
        "\n",
        "    # Ollama health check\n",
        "    ollama_healthy = check_ollama_health()\n",
        "    if ollama_healthy:\n",
        "        st.success(\"‚úì Ollama server: Running\")\n",
        "    else:\n",
        "        st.error(\"‚úó Ollama server: Not responding\")\n",
        "        st.button(\"Show restart instructions\", key=\"restart_btn\")\n",
        "        if st.session_state.get(\"restart_btn\"):\n",
        "            st.code(\"\"\"\n",
        "# Run in Colab:\n",
        "!pkill -f ollama\n",
        "import subprocess, os, time\n",
        "os.environ['OLLAMA_MODELS'] = '/content/drive/MyDrive/Ollama_Models'\n",
        "with open('/tmp/ollama.log', 'w') as f:\n",
        "    subprocess.Popen(['ollama', 'serve'], stdout=f, stderr=subprocess.STDOUT)\n",
        "time.sleep(10)\n",
        "            \"\"\")\n",
        "\n",
        "    st.markdown(f\"\"\"\n",
        "    **Ollama Models Path:**\n",
        "    ```\n",
        "    {os.environ.get('OLLAMA_MODELS', 'Not set')}\n",
        "    ```\n",
        "\n",
        "    **Current LLM Model:** {MODEL_NAME}\n",
        "\n",
        "    **Device:** {DEVICE.upper()}\n",
        "\n",
        "    **GPU Available:** {'‚úì Yes' if GPU_AVAILABLE else '‚úó No (using CPU)'}\n",
        "    \"\"\")\n",
        "\n",
        "    if GPU_AVAILABLE:\n",
        "        st.success(f\"üöÄ GPU Acceleration Enabled!\")\n",
        "        try:\n",
        "            import torch\n",
        "            gpu_name = torch.cuda.get_device_name(0)\n",
        "            st.info(f\"GPU: {gpu_name}\")\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        st.warning(\"‚ö†Ô∏è Running on CPU (slower)\")\n",
        "\n",
        "    st.header(\"Status\")\n",
        "\n",
        "    local_model_path = os.path.join(DRIVE_ROOT, \"local_model\")\n",
        "    if os.path.exists(local_model_path):\n",
        "        st.success(\"‚úì Local embedding model found in Drive\")\n",
        "    else:\n",
        "        st.warning(\"‚ö† Local model not found in Drive\")\n",
        "\n",
        "    if st.session_state['vectorstore'] is not None:\n",
        "        st.success(\"‚úì Document loaded\")\n",
        "    else:\n",
        "        st.info(\"‚Ñπ No document loaded yet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZhVqjwfVtBJ",
        "outputId": "6c89f6df-02e9-48ee-d997-30995ffdaaf1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PART 3: Run the App (Run after PART 2)\n",
        "# ========================================\n",
        "\n",
        "# Setup ngrok for public URL (optional, but recommended for Colab)\n",
        "!pip install -q pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "# Get ngrok auth token (sign up at https://ngrok.com for free)\n",
        "print(\"Get your free ngrok token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "ngrok_token = getpass.getpass(\"Enter your ngrok auth token: \")\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Start Streamlit in background\n",
        "!streamlit run app.py &>/dev/null &\n",
        "\n",
        "# Wait for Streamlit to start\n",
        "import time\n",
        "time.sleep(5)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"\\n‚úì App is running!\")\n",
        "print(f\"üåê Public URL: {public_url}\")\n",
        "print(f\"üì± Open this URL in your browser to access the app\")\n",
        "\n",
        "# Keep the tunnel alive\n",
        "ngrok_process = ngrok.get_tunnels()\n",
        "print(\"\\n‚ö†Ô∏è Keep this cell running to maintain the connection!\")\n",
        "\n",
        "# Alternative: Run without ngrok (local only)\n",
        "# Uncomment this if you don't want to use ngrok:\n",
        "# !streamlit run app.py --server.port 8501\n",
        "\n",
        "# ========================================\n",
        "# HELPER: Restart Ollama (Run this if Ollama crashes)\n",
        "# ========================================\n",
        "\n",
        "# Run this cell if you get \"Cannot assign requested address\" error\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"Restarting Ollama server...\")\n",
        "\n",
        "# Kill any existing Ollama processes\n",
        "!pkill -f ollama\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Ollama server\n",
        "os.environ['OLLAMA_MODELS'] = '/content/drive/MyDrive/Ollama_Models'\n",
        "\n",
        "with open('/tmp/ollama.log', 'w') as f:\n",
        "    ollama_process = subprocess.Popen(\n",
        "        ['ollama', 'serve'],\n",
        "        stdout=f,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        env=os.environ.copy()\n",
        "    )\n",
        "\n",
        "print(\"Waiting for Ollama to start...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Verify it's working\n",
        "result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Ollama server restarted successfully!\")\n",
        "    print(\"\\nAvailable models:\")\n",
        "    print(result.stdout)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Ollama may have issues. Check the log:\")\n",
        "    !tail -20 /tmp/ollama.log\n",
        "\n",
        "print(\"\\nüîÑ Now refresh your Streamlit app in the browser\")\n",
        "\n",
        "# ========================================\n",
        "# HELPER: Check Ollama Status\n",
        "# ========================================\n",
        "\n",
        "# Run this to check if Ollama is running\n",
        "!ps aux | grep ollama | grep -v grep\n",
        "print(\"\\nOllama models:\")\n",
        "!ollama list\n",
        "\n",
        "print(\"\\nTest Ollama connection:\")\n",
        "!curl -s http://localhost:11434/api/tags\n",
        "\n",
        "print(\"\\n\\nOllama log (last 20 lines):\")\n",
        "!tail -20 /tmp/ollama.lo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_B8GUnbV4Aa",
        "outputId": "6ffb976a-4abc-492f-9f02-a51969ec026f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get your free ngrok token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "Enter your ngrok auth token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "\n",
            "‚úì App is running!\n",
            "üåê Public URL: NgrokTunnel: \"https://07279fbedaa2.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "üì± Open this URL in your browser to access the app\n",
            "\n",
            "‚ö†Ô∏è Keep this cell running to maintain the connection!\n",
            "Restarting Ollama server...\n",
            "Waiting for Ollama to start...\n",
            "‚úì Ollama server restarted successfully!\n",
            "\n",
            "Available models:\n",
            "NAME         ID              SIZE      MODIFIED       \n",
            "gemma3:1b    8648f39daa8f    815 MB    37 minutes ago    \n",
            "\n",
            "\n",
            "üîÑ Now refresh your Streamlit app in the browser\n",
            "root       15085  1.2  0.3 6386716 45448 ?       Sl   01:17   0:00 ollama serve\n",
            "\n",
            "Ollama models:\n",
            "NAME         ID              SIZE      MODIFIED       \n",
            "gemma3:1b    8648f39daa8f    815 MB    37 minutes ago    \n",
            "\n",
            "Test Ollama connection:\n",
            "{\"models\":[{\"name\":\"gemma3:1b\",\"model\":\"gemma3:1b\",\"modified_at\":\"2025-10-01T00:39:43Z\",\"size\":815319791,\"digest\":\"8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"gemma3\",\"families\":[\"gemma3\"],\"parameter_size\":\"999.89M\",\"quantization_level\":\"Q4_K_M\"}}]}\n",
            "\n",
            "Ollama log (last 20 lines):\n",
            "tail: cannot open '/tmp/ollama.lo' for reading: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}